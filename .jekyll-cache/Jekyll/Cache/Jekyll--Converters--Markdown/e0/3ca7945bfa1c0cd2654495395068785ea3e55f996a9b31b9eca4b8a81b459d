I"M"<p>Back in the 1950s and 1960s, when cigarette advertising was legal in the United States, tobacco companies were major sponsors of television programs. In the late 60s, the <em>The Public Health Cigarette Smoking Act</em> bill was introduced in Congress, after the 1964 report by the Surgeon General found that lung cancer and chronic bronchitis are causally related to cigarette smoking. Interestingly, the two major tobacco brands which oligopolized the US tobacco industry then endorsed the law, and after President Nixon signed  the bill into law, during the first year profits (on average) for cigarette companies increased by $91 million!</p>

<p>Let’s look over why it was favourable for the tobacco industry when advertising was banned using a prisoner’s dilemma game. Prisoner’s dilemma is a formal game which is extensively used in game theory to understand decision making. It strips away the variations between specific situations in which people have a choice of cooperating or deceiving each other and has been called the <em>E. coli</em> of social psychology,  widely to research various topics such as oligopolistic competition and collective action to produce a collective good.[1]</p>

<h2 id="prisoners-dilemma">Prisoner’s dilemma</h2>

<p>In the words of Albert W. Tucker, who formalized the game and came up with it’s name, the game is the following,</p>

<blockquote>
  <p>Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict both on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the other by remaining silent. The possible outcomes are:</p>
</blockquote>

<blockquote>
  <ol>
    <li>If A and B each betray the other, each of them serves two years in prison</li>
    <li>If A betrays B but B remains silent, A will be set free and B will serve three years in prison</li>
    <li>If A remains silent but B betrays A, A will serve three years in prison and B will be set free</li>
    <li>If A and B both remain silent, both of them will serve only one year in prison (on the lesser charge).</li>
  </ol>
</blockquote>

<p>The generalized form for the above game is shown using the payoff matrix below</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: left">Cooperate</th>
      <th style="text-align: right">Defect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Cooperate</strong></td>
      <td style="text-align: left">R, R</td>
      <td style="text-align: right">S, T</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Defect</strong></td>
      <td style="text-align: left">T, S</td>
      <td style="text-align: right">P, P</td>
    </tr>
  </tbody>
</table>

<p>Where,</p>

<ul>
  <li>R is the payoff for cooperation</li>
  <li>T is the temptation payoff</li>
  <li>S is the sucker’s payoff</li>
  <li>P is the punishment payoff</li>
</ul>

<p>and the tuple <em>(x,y)</em> represents the payoff’s received by player 1 and 2 respectively. For example, if player 1 cooperates, where as player 2 defects, the payoff for the players is represented by the value in the top right cell, <em>(S, T)</em> i.e. player 1 receives the sucker’s payoff <em>S</em>, and player 2 gets the temptation payoff <em>T</em>. To be prisoner’s dilemma game in the strong sense, the following condition also must hold for the payoffs:</p>

<p><em>T</em> &gt; <em>R</em> &gt; <em>P</em> &gt; <em>S</em></p>

<p>If both the players are perfectly rational (their sole intent is to maximize the payoff for one self) and there is no opportunity for punishment or reward outside of the game, <strong>the dominant strategy (Nash equilibrium) is to always defect</strong>, regardless of what the other player chooses. This is backed by the reasoning that  B will either cooperate or defect. If B cooperates, A should defect, because going free is better than serving 1 year. If B defects, A should also defect, because serving 2 years is better than serving 3. So either way, A should defect. Parallel reasoning will show that B should defect.</p>

<p>This however, does not factor in repeat interractions with the same player. In real world scenarios, typically multiple iterations of such games are involved with the same opponent, and players retain information about their opponents behaviour in the past and switch strategies based on previous interactions. For instance, in certain strains of bacteria, when nutrients are scarce, two clonal lines (A clonal line is just multiple bacteria with the same genetic makeup), may aggregate into fruiting bodies in order to reproduce. Now there are 2 parts of the fruiting body, one being the stalk and the other part is the one that actually fruits (reproduces). During this process, the strain that attaches disproportionately to the fruiting section has a higher reproductive success, and there are often attempts by one strain to cheat (temptation payoff). But what is also seen is that next time around, the other strain does not coooperate with the cheater.</p>

<p>A large part of social behavior is built around organisms either trying to get away with something or spotting someone else do the same. And animals are usually very good at detecting when they are being cheated, more so than at detecting random acts of altruism. Deception might have an advantage in a single game of prisoner’s dilemma, but it’s no longer the dominant strategy in the iterative version (Caveat: This is only true when the total number of iterations of the game is not known).</p>

<h2 id="when-to-cooperate-and-when-to-defect">When to cooperate and when to defect</h2>

<p>Research on optimal strategies for the iterated prisoner’s dilemma (IPD) was kindled by Robert Axelrod in the 1980s. He organized a tournament in which participants have to choose their mutual strategy again and again, and have memory of their previous encounters. Axelrod invited academic colleagues all over the world to devise computer strategies to compete in an IPD tournament. The programs that were entered varied widely in algorithmic complexity, initial hostility, capacity for forgiveness, and so forth.</p>

<p>In these repeated games, Axelrod observed that there was one simple strategy which always outperformed all other strategies in the long run, irrespective of the other player’s strategy. The strategy is the following, start of by cooperating, and in subsequent rounds, simply mimic the oponent’s strategy, or in other words <strong>tit-for-tat</strong>.</p>

<p>The plot below shows the payoffs (the line represents the mean accross 5 rounds, each round had 200 games each, generated using the axelrod-python lib) for the different strategies.</p>

<p><img src="/assets/img/sample/axl-tournament-results.png" /></p>

<p>Axelrod analyzed the top performing strategies and determined the following characteristics for success,</p>

<ul>
  <li>Be nice: cooperate, never be the first to defect.</li>
  <li>Be provocable: return defection for defection, cooperation for cooperation.</li>
  <li>Don’t be envious: focus on maximizing your own ‘score’, as opposed to ensuring your score is higher than your ‘partner’s’.</li>
  <li>Don’t be too clever: or, don’t try to be tricky. Clarity is essential for others to cooperate with you.</li>
</ul>

<p>However, there is a vulnerability in tit-for-tat - real world systems are not 100% perfect. In case of a mistake in perception (the other player wanted to cooperate, but mistakenly sent the wrong signal), tit-for-tat can spiral into infinite loop of retaliation. Soon, as Axelrod began to introduce small amounts of signal error in the tournaments, another strategy overtook tit-for-tat as the winner - forgiving tit-for-tat. Forgiving tit-for-tat adds an occasional cooperation irrespective of being cheated in the previous move, to escape the death spiral and re-establish cooperation. However, this made this strategy vulnerable to being exploited playing against players who donot have forgiveness in them.</p>

<p>Soon it became apparent that there existed one even better strategy - start of with tit-for-tat, if and only if you go N rounds without being exploited by the other individual you switch to forgiving tit-for-tat - <strong>that’s deciding to establish trust</strong>.</p>

<p><img src="/assets/img/sample/george-bush-fool-me-twice.jpg" /></p>

<hr />
:ET